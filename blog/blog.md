---
layout: post
mathjax: true
---

Here, I will write about physics that is piquing my curiosity at any moment in time. Most of the writing is plasma physics/ fusion physics-themed. The entries will be either publicly accessible (+), somewhat technical (++), or specialist (+++). These views are my own, and don't necessarily reflect those of my employer. Whenever I publish a new scientific paper, I will also release a video explanation of the main paper ideas.

## (++) The SPARC machine, October 2nd 2020

Disclaimer: I have no affiliation with Commonwealth Fusion Systems or SPARC.

In recent years, but particularly in the past week, there has been some seismic activity in the fusion community. On September 30 2020, one of the fusion startups, Commonwealth Fusion Systems (CFS), released the physics basis for its fusion reactor, SPARC (Smallest Possible Advanced Reactor Concept). The seven peer-reviewed papers are Open Access on the Journal of Plasma Physics [website](https://www.cambridge.org/core/journals/journal-of-plasma-physics/collections/status-of-the-sparc-physics-basis). In my opinion, SPARC and the way CFS has made most of its physics basis publicly available is a rather big deal for fusion, which I will proceed to explain. If you are a card carrying member of the fusion community, you may wish to skip to (^). Otherwise, you might find the preceding background useful.

To understand the ramifications of SPARC's announcement, let us take a step back and see where progress in fusion is currently at. Since the 1940s until the late 1990s, experimental progress in fusion was fast: the triple product (the product of energy confinement time, density, and temperature), a key measure of plasma performance, had been increasing exponentially (its doubling time was actually faster than Moore's law for semiconductors). If progress had continued in the same fashion, we would have had a plasma with significant net gain by 2005-2010. Yet, here we are, in 2020, with no plasma with a significantly higher triple product (i.e. an order of magnitude) than that of the Joint European Torus (JET) or Japan Torus 60 (JT-60) in the 1990s. A lot has been written about this gap, so I shall not go into too much detail here, but in brief, the fusion community focused on building a single machine, ITER, which from design to first plasma (2025) will have taken around 40 years. ITER will not have its DT plasmas until at least the early 2030s. I am not going to offer judgement as to whether this was the right call, but simply state that this is where we are. Thus, to set the stage, we have no exponential experimental improvement in plasma performance in 20 years, in contrast to the 1940s-1990s. The lack of visible progress is not because of physics constraints, but rather the lack of new machines. Of course, it must be said that a huge amount of progress has been made in other areas of experimental fusion research (fueling, heating schemes, diagnostics, current drive, disruption mitigation, etc), but not in the triple product.

One of the most stated claims of fusion research is that fusion reactors will mitigate climate change by replacing carbon intensive sources of electricity with a reliable low carbon baseload supply (see my previous blog post 'Why Fusion?'). However, given the current timeline for a commerically viable fusion reactor (not a mass rollout, just the availability of such a reactor) going into the 2050s or 2060s, many are asking how much of an impact fusion will have, especially with other alternatives such as fission, solar, wind, etc. I don't want to get into the weeds of this debate (renewables versus nuclear), but simply pose the following maxim: the sooner we have a commerically viable fusion reactor, the more useful it will be for climate change. So let us all agree that a reactor in the 2030s will be better than one in the 2060s.

(^) Since the late 1990s, a plethora of 'fusion startups' have popped up. None of them have had the same impact in the fusion community as SPARC, even Tokamak Energy, which is notionally similar to SPARC. So what is SPARC, and why is it such a big deal?

SPARC is a 'medium' sized tokamak (it's essentially the same size as the Korean K-STAR device) that relies on recent advances in high temperature superconductors to generate on-axis magnetic fields that are almost four times as large as JET (Creely, 2020), and over twice that of ITER. Apparently, the joke at CFS is that high magnetic fields are the solution to all of our woes, which in this case, they may well be. High magnetic fields are claimed to be very useful for fusion, which will herald the creation of much smaller, cheaper, and faster-to-build fusion reactors. A scaling that you will see thrown around a lot is one for the total fusion power,
\begin{equation}
P_{fus} \sim \frac{ \beta_N^2 B_t^4 R^3 }{q^2 A^4} \;\;\;\;\;\;\; (1).
\end{equation}
This scaling is one of the main justifications that SPARC and other high field devices have used to justify their claims of building smaller reactors with high magnetic fields. Here, $P_{fus}$ is the fusion power, $\beta_N$ is a dimensionless parameter that measures the ratio of thermal to magnetic energy, $B_t$ is the toroidal magnetic field strength, $R$ is the major radius of the device, $q$ is the safety factor, and $A = R / a$ is the aspect ratio, where $a$ is the minor radius of the device. While these scaling laws are undoubtedly useful, it is not always true that all the parameters can be independently varied in an actual design. For example, the safety factor $q$ roughly measures the ratio of the toroidal field strength to the poloidal field strength. If you increase $B_t$, which is controlled by the external toroidal magnetic field coils, you need to drive more toroidal current to generate the poloidal magnetic field such that $q$ is kept constant; clearly you cannot do this indefinitely. Similar arguments apply to other parameter pairs. However, for a simplification, let us assume that the scaling in equation (1) is self-similar, and that all the parameters can be varied independently.

Evidently, according to equation (1), if one doubles the magnetic field strength, the fusion power purportedly increases by a factor of 16. This seems almost too good to be true; why not just take $B_t \to \infty$ on all fusion designs and have as much fusion power as we want? The short answer is that we have taken $B_t \to \infty$ within the limits of superconducting magnet technology and engineering know-how that was available at the time of design. For example, ITER was actually designed to be roughly the smallest possible device that would have a gain factor of $Q = 10$, with the strongest possible magnetic field available at the time (about 5 Tesla on axis). However, intriguingly, even back in the 1980s we had the technology to build non-superconducting magnets that could generate $B_t \approx $

Materials...

Arguably, in the meantime, strides have been made in computational and theoretical understanding of fusion physics, such that SPARC might not have been possible twenty years.
